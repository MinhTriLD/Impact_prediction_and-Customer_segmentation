{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7686254,"sourceType":"datasetVersion","datasetId":4485158}],"dockerImageVersionId":30646,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/mobile-app-technical-specifications-30560f5b-da8e-46c6-aef9-5ca160f44d46.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20240217/auto/storage/goog4_request&X-Goog-Date=20240217T072243Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=3051d7c4af7fdec989faee1294e581ac16e7fc7b8d8f5e7f4b716f61e9b0e7b579298ec73880e2b5f0076c5bc745e2133c278b85d00414841fe4be6225960b40e843c1462133080b1b6b5a4bfcbb9358cceb7b7e31bf89e101c07473abbef328d51a19b84607e5e8f263dba0ae499827a98e302115e1e280fc1c32873fcda239db45b3697ae73f38ff2df93ac3ecd8ea106908c4e8114676aed922bf7e6364e0cee7574c946d1adeb8f4aa8970dcb19cbc04b6b2253d76b62234ae8ed010854529aea08c9ad2f91dd42f694dbb32c7c09feb8a25052ad5c8d8d51ef587d4ba06f82920fed6c4d36ff4877cb050145ce044e784a04f64efa0dcab2bf876acd40e","timestamp":1708154609317}]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <b><u> Fashion boutique – Trade marketing\n## - How web traffic impacts on business performance?\n## - Customer segmentation\n\n## <b><u>Abstract:</u></b>\nA Company has been operated it's business as a fashion boutique chain since the March of 2019. They are possessing stores in each country’s capital: UK (London); FR (Paris); IT (Milan); GER (Berlin). Since the end of 2019, to enhance the business performance, BoDs have intend to invest in applying technical solutions such as website and big data into their business.\n    \nThis company kicked off a website at the beginning of 2020. This is an experiment for the marketing promotion, thus, they just fund a static web with a few simple functiom for only the display of the boutiques information, product detail, pictures and purchasing contact. The website has not been combined the functions to process the data input by web users to determine web user and collect vital data. This web just has able to count distinct user ('users'), page view ('pageviews') and unique page view based on loading within a certain duration ('uniquePageviews') for each web page of each product. \n    \nTo make decision for investing the web upgraded to  dynamic functions matching for the E-commerce, the board of management requested a coverage of the website traffic's impact on business performance. Another request is to determine personal group of current customers. \n    \nThe material is supplied to complete this mission, includes the documents in type of Excel file (.xlsx), which record the traffic of each each 2020's month and a compound file containing sheets where data of customers, items and transactions. \n    \nSome feature has been encoded or changed to conceal the identification.\n    \n### <b><u>Request 1:</u></b> How web traffic impacts on business performance?\n    \n### <b><u>Request 2:</u></b> Customer segmentation.\n    \n","metadata":{}},{"cell_type":"markdown","source":"## <b><u>Input data:</u></b>","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"id":"aEXo6Od3-WIb","executionInfo":{"status":"ok","timestamp":1708178426176,"user_tz":-420,"elapsed":4,"user":{"displayName":"Trí Lê","userId":"10896021377759287168"}},"outputId":"0ce8fe63-fa10-413f-d73a-ab50181b8035","execution":{"iopub.status.busy":"2024-02-24T19:14:09.958002Z","iopub.execute_input":"2024-02-24T19:14:09.958339Z","iopub.status.idle":"2024-02-24T19:14:10.298893Z","shell.execute_reply.started":"2024-02-24T19:14:09.958311Z","shell.execute_reply":"2024-02-24T19:14:10.297791Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/fashion-boutique-chain/2020_08.xlsx\n/kaggle/input/fashion-boutique-chain/2020_06.xlsx\n/kaggle/input/fashion-boutique-chain/2020_03.xlsx\n/kaggle/input/fashion-boutique-chain/2020_01.xlsx\n/kaggle/input/fashion-boutique-chain/customers (UK).xlsx\n/kaggle/input/fashion-boutique-chain/2020_04.xlsx\n/kaggle/input/fashion-boutique-chain/2020_05.xlsx\n/kaggle/input/fashion-boutique-chain/2020_12.xlsx\n/kaggle/input/fashion-boutique-chain/2020_02.xlsx\n/kaggle/input/fashion-boutique-chain/2020_09.xlsx\n/kaggle/input/fashion-boutique-chain/2020_11.xlsx\n/kaggle/input/fashion-boutique-chain/2020_07.xlsx\n/kaggle/input/fashion-boutique-chain/2020_10.xlsx\n","output_type":"stream"}]},{"cell_type":"code","source":"# Reading all the files\nraw_path = '/kaggle/input/fashion-boutique-chain/'\ncustomers = pd.read_excel(raw_path + 'customers (UK).xlsx',sheet_name ='customer info')\nitems = pd.read_excel(raw_path + 'customers (UK).xlsx',sheet_name ='Items')\ntransactions = pd.read_excel(raw_path + 'customers (UK).xlsx',sheet_name ='Customer transactions')\ndf01 = pd.read_excel(raw_path + '2020_01.xlsx')\ndf02 = pd.read_excel(raw_path + '2020_02.xlsx')\ndf03 = pd.read_excel(raw_path + '2020_03.xlsx')\ndf04 = pd.read_excel(raw_path + '2020_04.xlsx')\ndf05 = pd.read_excel(raw_path + '2020_05.xlsx')\ndf06 = pd.read_excel(raw_path + '2020_06.xlsx')\ndf07 = pd.read_excel(raw_path + '2020_07.xlsx')\ndf08 = pd.read_excel(raw_path + '2020_08.xlsx')\ndf09 = pd.read_excel(raw_path + '2020_09.xlsx')\ndf10 = pd.read_excel(raw_path + '2020_10.xlsx')\ndf11 = pd.read_excel(raw_path + '2020_11.xlsx')\ndf12 = pd.read_excel(raw_path + '2020_12.xlsx')","metadata":{"id":"M4jDE0Vn8f92","execution":{"iopub.status.busy":"2024-02-24T19:14:10.300679Z","iopub.execute_input":"2024-02-24T19:14:10.301057Z","iopub.status.idle":"2024-02-24T19:14:19.392651Z","shell.execute_reply.started":"2024-02-24T19:14:10.301032Z","shell.execute_reply":"2024-02-24T19:14:19.391055Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Gathers 'traffic' dataset of the year 2020\ntraffic = pd.concat([df01,df02,df03,df04,df05,df06,df07,df08,df09,df10,df11,df12])","metadata":{"id":"4Dbo5FwO9v-F","execution":{"iopub.status.busy":"2024-02-24T19:14:19.393693Z","iopub.execute_input":"2024-02-24T19:14:19.394154Z","iopub.status.idle":"2024-02-24T19:14:19.402090Z","shell.execute_reply.started":"2024-02-24T19:14:19.394127Z","shell.execute_reply":"2024-02-24T19:14:19.400692Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## <b><u>II. Introduction:</u></b>\nThis paper provides a full procedure of a data analysis project from 'init' to 'predict' with the great emphasis on business accumen for satisfying the request assigned. Over and above, the result of this project gives the acknowledge about how website application impact on business performance and the classification of customers to establish the business strategic plans. Noticably, this report should be implemented periodically to measure the performance and keep the figures been upto date.\n\nThe assigned requests is too general to determine the features which need to be measured and considered for the report output. By the knowledge about business management and digital marketing, I am able to affirm which features relevant to fators and consequence in the question. Below is the discussion and predict identical variables.\n\n**Request 1:** How web traffic impacts on business performance? \n    \n- Features relevant to web traffic are the count of users, page view which  is the number of times a web page is loaded, unique page view which is the number of unique times a web page loaded within a specific time period, and existing duration.\n    \n- Business performance is too general,because it includes the whole business's aspects. Business performance would be specified sales performance due to digital marketing only being able to impact on the sale of business. Therefore, it is certainly expressed by the indicators of revenue, avg revenue per unit, sales volume, customer volume, vonversiom rate, etc.\n        \n    These variables are numerical and datetime, which used respectively to express quantity and timestamp. \n        \n**Request 2:** Customer segmentation: \n    \nIn addition to segmenting customers by the categorical variables provided in the original dataset such as geography, gender, etc., customer segmentation can also be grouped by numerical and time data. For example, independent variables for grouping are age, relationship time, spending turnover, and some other models like RFM, etc.\n\nThe tools that facilitate this analysis are libraries supported by Python and Machine Learning models.\n\n**Flow chart:**\n\nDetermine the contextual situation and understand the requests   --->   Determine detail of independent and dependent variable   --->   Select sufficient tools   --->   Overview data and determine incorrect and feature for conversion or optimization   --->   Process and feature data   --->   Analyze exploratorily data (EDA)   --->   Select feature and model   --->   Train model   --->   Evaluate model   --->   Discuss analysis results and Conclude. ","metadata":{"id":"FTN5pPAb8juC"}},{"cell_type":"markdown","source":"## <b><u>III. Dataset overview:</u></b>\n\n![Data structure](https://i.imgur.com/Rx0tNx0.png)\n\nThe Entity Ralationship Diagram (ERD) of the material dataset is conducive to regcognize the relationship among tables or entities was expressed obviously as well as the feature detail in each table. \n\nCurrently, the dataset has possessed three dim tables showing the figures of three entities, include items, customers and web traffic. Additionally, the dataset has had one fact table which contains the real-time historical events relevant to the success transactions with customers. It is a striking feature that the dim table of web traffic could be integrated into products table, meanwhile the fact table, transaction, could be split into 2 tables being the dim table order and the fact table transaction by the key being order ID with the relationship respectively being one-to-many. Below is the references of entity relationship in this dataset:\n\n* Ref: traffic.product - item.Product // one-to-one\n\n* Ref: customer.ID < transaction.CustomerID // one-to-many\n\n* Ref: item.ItemID < transaction.ItemID // one-to-many\n\nThe description of feature names is not too intensive to conceive the definition of them. However, there is some technical feature which need the explaination in this dataset, are page URL, users, pageviews and unique pageviews, below is their dictionary:\n\n* Page URL ('Page_URL') is the link leading to the product's web page.\n* Users ('users') is the number of distinct viewers accessing the page URL.\n* Page views ('pageviews') is the count of the web page's loading.\n* Unique page views ('uniquePageviews') is the count of the access in the page URL by an user in a specific period.\n\n#### To determine the existing issues in the material dataset to establish the strategy of data processing, a statistical table which gives information about the number of the common issues in each feature, was created below. Additionally, the detail of features is also expressed. Moreover, it is no doubt that the value samples in each feature should be also observed to reacg the insight.","metadata":{}},{"cell_type":"code","source":"transactions.head(), customers.head(), traffic.head(),  items.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-24T19:14:19.403274Z","iopub.execute_input":"2024-02-24T19:14:19.403742Z","iopub.status.idle":"2024-02-24T19:14:19.437335Z","shell.execute_reply.started":"2024-02-24T19:14:19.403697Z","shell.execute_reply":"2024-02-24T19:14:19.436632Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(   OrderID  CustomerID  ItemID TransactionDate   Channel\n 0        0           0     352      2020-03-21  In Store\n 1        0           0    3433      2020-07-14  In Store\n 2        0           0   11162      2020-08-09  In Store\n 3        0           0   13011      2020-12-07  In Store\n 4        0           0   13885      2020-11-08  In Store,\n    ID              FirstName               LastName              Country  \\\n 0   0     V0.296680287495188     L0.104646531512644          FR - France   \n 1   1     D0.793097101838541   Law0.141693355411763        GER - Germany   \n 2   2   Ker0.141418247925814    Ng0.753960335680345          FR - France   \n 3   3   Fik0.950054552966336     F0.590961171612745  UK - United Kingdom   \n 4   4  Iona0.294287981536498  Ison0.826191754811968           IT - Italy   \n \n   DateJoined Gender   Birthday Newsletter  \n 0 2020-03-17    NaN 1968-02-03          N  \n 1 2020-03-20      M 2009-10-06          Y  \n 2 2020-03-21      F 1990-08-04          Y  \n 3 2020-03-21      M 1974-07-24          N  \n 4 2020-03-21      M 1981-08-13          N  ,\n                              Page URL   users  uniquePageviews  pageviews  \\\n 0  /2020/1/032irview0.686128260621012  5669.2           5777.8     6286.4   \n 1  /2020/1/070ttream0.518887735674677   359.8            370.4      403.4   \n 2  /2020/1/070htream0.333307794468401   587.6            614.2      657.6   \n 3  /2020/1/100Grseys0.271522111052549  1284.0           1308.6     1385.4   \n 4  /2020/1/100[nside0.645837365801341  1846.0           1880.8     2025.0   \n \n               Brand Posted On (DD/MM/YYYY)  \n 0  Ki)D3jDmA,RIP68X    2020-01-10 16:56:13  \n 1  GO4582ey<S!+k1VE    2020-01-10 05:04:35  \n 2  G.Kb^jz*soY!(-4Q    2020-01-16 23:27:08  \n 3  Dr|vm[-5p~56Y\\mk    2020-01-17 12:32:24  \n 4  Dr|vm[-5p~56Y\\mk    2020-01-23 05:21:08  ,\n    ItemID                     Product             Brand  SellPrice  CostPrice\n 0       1  032irview0.686128260621012  Ki)D3jDmA,RIP68X        943        359\n 1       2  070ttream0.518887735674677  GO4582ey<S!+k1VE        717        207\n 2       3  070htream0.333307794468401  G.Kb^jz*soY!(-4Q        739        199\n 3       4  100Grseys0.271522111052549  Dr|vm[-5p~56Y\\mk        532        262\n 4       5  100[nside0.645837365801341  Dr|vm[-5p~56Y\\mk        593        392)"},"metadata":{}}]},{"cell_type":"markdown","source":"By observing the values sample, the coversion could be applied for the correct feature. The gained insight includes:\n\n1. Firstname and lastname are stored in customer can be concatenated to full name.\n2. 'Birthday' and 'DateJoined' of customers with the'Posted On (DD/MM/YYYY)' of traffic are date categorical feature, are not time series, and could be converted to time duration, which is numerical type, and simplifies the measurement. \n3. The dim table 'traffic' is able to to connect with the dim table 'products' by the fourth string of each 'Page_URL' value split by '/', which is equal to each product's encoded name. Moreover, the number of distinct values in 'traffic' is also equal to the figures in products. In conclusion, each instance in 'traffic' is matching with that in 'products'.\n4. The identificational features of the customers and the products has been encoded to protect customer security and trade secret.\n","metadata":{}},{"cell_type":"code","source":"# List dataset\ndatasets = [traffic, customers, items, transactions]\n# List dataset name\ntitles = ['traffic','customers','items','transactions']\n\n# Creates an empty list to store findings\nsummary_list = []\n\n\n# Implements a loop to iterate over the data frames in the lists\n# and construct a summary table for each one\nfor title, df in zip(titles, datasets):\n    # Creates 'sumary' table with 3 columns\n    summary = pd.DataFrame({'dataset':title,\n                            'data_type':df.dtypes,\n                            'quantity':len(df),\n                            'distinct_value': df.nunique(),\n                           'dup_row':df.duplicated().sum(),\n                           'dup_row%':df.duplicated().mean()})\n    # Creates an empty list to store counts of duplicate rows and null values for each column\n    # Assists in making decision for data exploring and preprocessing\n    duplicated_value = []\n    null_value = []\n    min_value = []\n    max_value = []\n    #quantile_25=[]\n    # Iterates through each column in the DataFrame\n    for column in df.columns:\n        # Counts the number of the duplicated and the null then appends them respectively\n        # in the matching list.\n        duplicated_value.append(df.duplicated(subset=[column]).sum())\n        null_value.append(df[column].isnull().sum())\n        if df[column].dtype == 'float64' or df[column].dtype == 'int64' or df[column].dtype == 'datetime64[ns]':\n            min_value.append(df[column].min())\n            max_value.append(df[column].max())\n        else:\n            min_value.append(np.nan)\n            max_value.append(np.nan)\n        #quantile_25.append(df[column].quantile(0.25))\n    # Add columns and attach value list in the 'sumary' table\n    summary['dup_value'] = duplicated_value\n\n    summary['dup_value%'] = summary['dup_value'] /len(df) *100\n\n    summary['null_value']= null_value\n\n    summary['null_value%'] = summary['null_value']/len(df) *100\n    \n    summary['min_value'] = min_value\n    \n    summary['max_value'] = max_value\n\n    #summary['quantile_25']=quantile_25\n\n    # Converts the index of the summary DataFrame to a column named column and\n    # appends it to the summary_list\n    summary_list.append(summary.reset_index())\n\n# Concatenates all summary DataFrames in the summary_list vertically\n#into a single DataFrame named combined_summary.\ncombined_summary = pd.concat(summary_list)\n\ncombined_summary= combined_summary.rename(columns = {'index':'column'} )\ncombined_summary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This table shows the number and the proportion of the null and duplicated values in each column of dataframes. Additionally, it points out the data type of values in each column and the number of duplicated rows in each dataset.</li>\n\nhis table is conducive to penetrating dataset and intending to process data by combining the description of columns. The findings are obtained, include:\n        <ol>\n            <li>The numeric indicators of web traffic are the count, which are 'uniquePageviews','pageviews' and 'users', would be set the form of positive integer.\n            </li>\n            <li>The 'transactions' dataset is the fact dateset which store the transactional information with the customers of the store.\n            </li>\n            <li>There are three columns which own binary values (0 and 1), are respectively 'Gender','Newsleett','Channel'. \n            </li>\n            <li>There is not any duplicated row.\n            </li>\n            <li>The unique keys of each dataset are not duplicated.\n            </li>\n            <li>There are 251 missing values in 'Gender' of 'cust' dataset taking a large portion of this dataset. This has been a sensitive personal information in the current time. So they will be replaced by 'bisexual'.\n            </li>  \n        \nWith the above judgements about the dataset, the following will be implemented respectively to solve the issues for earning the useful dataset.\n\n\n\n    \n        \n    ","metadata":{"id":"L3VFwmCX8juF"}},{"cell_type":"markdown","source":"## <b><u>IV. Data preprocessing:</u></b>\n### 1. <b><u>Convert data:</u>   \n### <b><u> FirstName and LastName:</u></b> convert to 'FullName'.","metadata":{"id":"cMkMEFz5Q8b7"}},{"cell_type":"code","source":"# Adds fullname concatenated by 'FirstName' and 'LastName'\ncustomers['FullName'] = customers['FirstName']+' '+customers['LastName']\ncustomers.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><u>'Birthday', 'DateJoined' and 'Posted On (DD/MM/YYYY)':</u></b> convert to integer.","metadata":{}},{"cell_type":"code","source":"# Imports datetime library\nfrom datetime import date\n# Extract the present date of the dataset by the most recent day of transaction date\ndate_today = pd.to_datetime(transactions['TransactionDate']).max() + pd.Timedelta(days=1)\nprint(f'Today: {date_today}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adds matching time duration values of customers accounted by date_today and identical birthday in 'Age' column\ncustomers['Age']= customers['Birthday'].apply(lambda x: (date_today - x).days//365)\ncustomers['relationship_duration'] = customers['DateJoined'].apply(lambda x: (date_today - x).days)\ntraffic['existing_duration']= traffic['Posted On (DD/MM/YYYY)'].apply(lambda x: (date_today - x).days)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customers.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traffic.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><u>'Page_URL':</u></b> convert to product.","metadata":{}},{"cell_type":"code","source":"traffic['Product']=traffic['Page URL'].str.split('/').str[3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traffic.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><u>'uniquePageviews', 'pageviews' and 'users':</u></b> convert to integer","metadata":{}},{"cell_type":"code","source":"# Rounds the float values at the count columns\ntraffic[['uniquePageviews','pageviews','users']]=traffic[['uniquePageviews','pageviews','users']].round(0)","metadata":{"id":"pGg9PPYFRJYK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traffic.head()","metadata":{"id":"ZZVd_-9YRJdh","executionInfo":{"status":"ok","timestamp":1708178469300,"user_tz":-420,"elapsed":454,"user":{"displayName":"Trí Lê","userId":"10896021377759287168"}},"outputId":"2bb86fb0-9252-45fb-f405-7b4dd8cebdad","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transforms value type to integer.\ntraffic[['uniquePageviews','pageviews','users']]=traffic[['uniquePageviews','pageviews','users']].astype(int)","metadata":{"id":"aGHvLV6KRJU6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. <b><u>Process missing values:</u></b>\n","metadata":{"id":"_GtmnIvV8juY"}},{"cell_type":"code","source":"customers['Gender'].head()","metadata":{"id":"8sSAlxlxTSvb","executionInfo":{"status":"ok","timestamp":1708178469301,"user_tz":-420,"elapsed":16,"user":{"displayName":"Trí Lê","userId":"10896021377759287168"}},"outputId":"f5405596-edc3-4484-d82b-ecf7122a0880","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fills all missing values (nan) by 'B' meaning bisexual.\ncustomers['Gender']=customers['Gender'].fillna('B')","metadata":{"id":"kT0xNS478juY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <b><u>V. Exploratory Data Analysis (EDA):</u></b>\n\nEDA in this project will be executed by data visualization - charts, which are depended on data type, and serves materially the observation \nof numerous instance by two or more dimensions for gaining their correlation. It assist effectively  to inspect smoothly the direction of each feature by others. \n\nThe analysis will be initiated by observing the data distribution which intents on the sight of the standard deviation, the percentile and the outline  of the values in each feature for attaining the data dispersion to  evaluate the sufficiency of each feature. The following is the correlation between the features and the research objects for figuring out the proper fearures vitally of the predicted models. \n\nIt is a striking feature that EDA should be implemented by the fact tables to determine the elements of frequency, recency, quantity, quality, trendy and etc. In addition, the features with the same data type could be transformed to the same graph. The project's data set has three data types.\n\nHowever, in terms of the design and function of this dataset, it's fact table is the transaction table which express the information of a sold item. This data could be ranked ordinally.\n\nit assists identify the vital feature affect to research object.\n\n\nObject1 : Customer volum (y1)\nObject2 : Product sales volume (y2)","metadata":{"id":"G2B5kNMT8jub"}},{"cell_type":"code","source":"# Merges 'traffic' tables into 'items' table  by 'Product' value\nitems = pd.merge(traffic,items, on = 'Product',how= 'left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merges the dim tables into the new fact table transaction, namely 'trans_detail'\ntransactions = pd.merge(transactions,customers,left_on='CustomerID',right_on='ID',how='left')\ntransactions = pd.merge(transactions,items,on='ItemID',how='left')\ntransactions = transactions.drop([\"ID\"], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sorts the DataFrame by the 'TransactionDate' column\ntrans_detail = transactions.sort_values(by='TransactionDate')\n\n# Resets the index of the sorted DataFrame\ntrans_detail.reset_index(drop=True, inplace=True)\n\n# Adds a new column for numbering the rows\ntrans_detail['ordinal'] = trans_detail.index + 1\n\n# Prints the first 5 rows of the sorted DataFrame with row numbers\nprint(trans_detail.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After concatenating all tables in dataset, there is not any missing value.\n\nThis points out the data collecting has been concerned on by the store.","metadata":{}},{"cell_type":"code","source":"trans_detail.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After concatenating all tables in dataset, there is not any missing value, the cause is due to the data collecting being concerned on by the store. However, there is one duplicated column of brand due to it's presence ìn two table.","metadata":{}},{"cell_type":"code","source":"# Replace two current incorrect brand name column by new column, namely 'Brand'\ntrans_detail['Brand'] = trans_detail['Brand_y']\ntrans_detail = trans_detail.drop(['Brand_y','Brand_x'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After merging tables into the fact table, the columns are arranged by their data type to analyze specifically.","metadata":{}},{"cell_type":"code","source":"# Creates column groups with the same datatype\ncategorical_columns = trans_detail.select_dtypes(exclude=['number', 'datetime']).columns\nnumerical_columns = trans_detail.select_dtypes(include='number').columns\ndatetime_columns = trans_detail.select_dtypes(include='datetime').columns\n\n# Checks results\nprint(categorical_columns,numerical_columns,datetime_columns)","metadata":{"id":"V9I945Og8jua","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Imports libraries to visualize statistical date\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math","metadata":{"id":"_hkNBrck8jub","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <b><u>1. Data distribution</u></b>\n### A. <b><u>Datetime feature:</u></b>\n\nThis dataset has four columns data, include 'TransactionDate','DateJoined','Posted On (DD/MM/YYYY)' and 'Birthday'.\n\nHowever, These columns are used to store the categorical value of the objects. \n\nThey record the date that instances of the object added to itself. Particularly, they help determining the existing time duration of object sufficiently.\n\n* 'TransactionDate' give information about the timestampt when each transaction on a product item was done. It records events in the time series\n\n* 'DateJoined' give information about the timestampt when customers perform the first purchasing order. by each customer.\n\n* 'Birthday' give information about the timestampt when customers begin breathing.\n\n* 'Posted On (DD/MM/YYYY)' show the date when each web page of  each product is published by the website of the store. \n\n#### <b><u>Transaction date</u></b>","metadata":{}},{"cell_type":"code","source":"# Sets up the figure size and create subplots\nplt.figure(figsize=(18, 10))\n    \n# Group the data by date and calculate the sum of installs and count of apps\ngroupby_date = trans_detail.groupby(['TransactionDate'], as_index=False).agg(product_sales_vol=('ordinal', 'count'),\n                                                                            revenue= ('SellPrice', 'sum'),\n                                                                             cust_vol=('CustomerID', 'nunique'),)\n\nsns.scatterplot(x='TransactionDate', y='product_sales_vol',  hue='revenue', size='cust_vol',\n                data=groupby_date, palette='winter', edgecolor='black')\n\nsns.lineplot(x='TransactionDate', y='product_sales_vol', data=groupby_date, color='orchid', linewidth=1)\n    \nplt.xlabel('Transaction Date')\nplt.ylabel('Product sales volume')\nplt.title('Scatter Plot by Transaction Date')\nplt.xticks(rotation=0)\nstart_date = pd.Timestamp('2020-01-01')\nend_date = pd.Timestamp('2020-03-31')\n\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This scatter expresses that the business activity of the store had maintained sustainability during the period of 2020. \n\nThe main reason is that during the period of 2020, the data is distributed stationary commonly inside a cluster from above 60 unit to under 90 units with min and max respectively just under 50 and 110 units. There are some timestamp when the sale witnessed the breakthrough indicators of the business operation. However, most points have the mean and the extent of variance and covariance be remained less the difference during this period. \n\nIn conclusion, the datatime feature is stationary, it means the models could be established easier with high accuracy follow this feature.","metadata":{}},{"cell_type":"markdown","source":"#### <b><u>Date joined</u></b>","metadata":{}},{"cell_type":"code","source":"# Sets up the figure size and create subplots\nplt.figure(figsize=(18, 10))\n    \n# Group the data by date and calculate the sum of installs and count of apps\ngroupby_date = trans_detail.groupby(['DateJoined'], as_index=False).agg(product_sales_vol=('ordinal', 'count'),\n                                                                            revenue= ('SellPrice', 'sum'),\n                                                                             cust_vol=('CustomerID', 'nunique'),)\n\nsns.scatterplot(x='DateJoined', y='product_sales_vol',  hue='revenue', size='cust_vol',\n                data=groupby_date, palette='winter', edgecolor='black')\n\nsns.lineplot(x='DateJoined', y='product_sales_vol', data=groupby_date, color='orchid', linewidth=1)\n    \nplt.xlabel('Date Joined')\nplt.ylabel('Product sales volume')\nplt.title(f'Scatter Plot by date joined')\nplt.xticks(rotation=0)\nstart_date = pd.Timestamp('2020-01-01')\nend_date = pd.Timestamp('2020-03-31')\n\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This chart shows the number of the sold items that customers with the same date joined paying for. During this time series, the data distribution had less the varied. Because, the instances had been scateered followed a inline gap with a split duration, and existed a few outliners during the period.\n\nThe mean, variance and covariance of each instances are too different to build a good model easily. This should not be used as a time series feature. ","metadata":{}},{"cell_type":"markdown","source":"#### <b><u>Birthday</u></b>\n","metadata":{}},{"cell_type":"code","source":"# Sets up the figure size and create subplots\nplt.figure(figsize=(18, 10))\n    \n# Group the data by date and calculate the sum of installs and count of apps\ngroupby_date = trans_detail.groupby(['Birthday'], as_index=False).agg(product_sales_vol=('ordinal', 'count'),\n                                                                            revenue= ('SellPrice', 'sum'),\n                                                                             cust_vol=('CustomerID', 'nunique'),)\n\nsns.scatterplot(x='Birthday', y='product_sales_vol',  hue='revenue', size='cust_vol',\n                data=groupby_date, palette='winter', edgecolor='black')\n\nsns.lineplot(x='Birthday', y='product_sales_vol', data=groupby_date, color='orchid', linewidth=1)\n    \nplt.xlabel('Birthday')\nplt.ylabel('Product sales volume')\nplt.title(f'Scatter Plot by Birthday')\nplt.xticks(rotation=0)\nstart_date = pd.Timestamp('2020-01-01')\nend_date = pd.Timestamp('2020-03-31')\n\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is the date which provides the persoanal information, it is used to group individual object by their age - a demographic object.\n\nIn addition, during the period of 2020, the data is distributed stationary commonly inside a cluster from 1 unit to just above 60 units with min and max respectively at 1 and 90 units. There are some timestamp when  the number of values witnessed the breakthrough customer feature. However, most points have the mean and the extent of variance and covariance be remained less the difference during this period.\n\nIn conclusion, the datatime feature is stationary, it means the models could be established easier with high accuracy follow this feature.","metadata":{}},{"cell_type":"markdown","source":"#### <b><u>Posted On (DD/MM/YYYY)</u></b>","metadata":{}},{"cell_type":"code","source":"# Sets up the figure size and create subplots\nplt.figure(figsize=(18, 10))\n    \n# Group the data by date and calculate the sum of installs and count of apps\ngroupby_date = trans_detail.groupby(['Posted On (DD/MM/YYYY)'], as_index=False).agg(product_sales_vol=('ordinal', 'count'),\n                                                                            revenue= ('SellPrice', 'sum'),\n                                                                             cust_vol=('CustomerID', 'nunique'),)\n\nsns.scatterplot(x='Posted On (DD/MM/YYYY)', y='product_sales_vol',  hue='revenue', size='cust_vol',\n                data=groupby_date, palette='winter', edgecolor='black')\n\nsns.lineplot(x='Posted On (DD/MM/YYYY)', y='product_sales_vol', data=groupby_date, color='orchid', linewidth=1)\n    \nplt.xlabel('Posted On (DD/MM/YYYY)')\nplt.ylabel('Product sales volume')\nplt.title(f'Scatter Plot by Posted On (DD/MM/YYYY)')\nplt.xticks(rotation=0)\nstart_date = pd.Timestamp('2020-03-16')\nend_date = pd.Timestamp('2020-03-17')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"During the period of 2020, the data is distributed stationary commonly inside a cluster from 1 unit to 5 units with max at 9 units. There are some timestamp when the number of values witnessed the breakthrough of web page. However, most points have the mean and the extent of variance and covariance be remained less the difference during this period.\n\nIn conclusion, the datatime feature is stationary, it means the models could be established easier with high accuracy follow this feature.\n\nThese charts show that the products were posted on the store's website, have tend to be sold more than others and also have caught more customers and orders than the others.","metadata":{}},{"cell_type":"markdown","source":"### B. <b><u>Categorical feature:</u></b>\n\nCategorical features just are able to classification, assist to selection so their analysis is done at observation to gain the dominant of each class.\nIn addition, it help conclude the effect of categorical features on the number of customer and sold items.\n\nMoreover, the classification features which express the indentical and unique values of each entity, would be removed when implementing EDA.\n\nThe indentical and unique values of the entities include 'FirstName', 'LastName', 'FullName', 'Page_URL', and 'Product'. Additionally, 'Brand' is also known as an indentical and unique values of the entities, suches as supplier.","metadata":{"id":"r7xG0Vlm8jud"}},{"cell_type":"code","source":"categorical_columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filtered_categorical_columns = categorical_columns.drop(['FirstName', 'LastName', 'FullName', 'Page URL', 'Product', 'Brand'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the figure size\nfiguresize = (20, 7)\n\n# Define the number of columns and rows for subplots\ncols = 4\nrows = math.ceil(len(filtered_categorical_columns) / cols)\n\n# Create subplots\nfig, axes = plt.subplots(rows, cols, figsize=figuresize)\n\n# Iterate over the subplots and plot countplots for each categorical column\nfor i, ax in enumerate(axes.flat, start=0):  # Start index adjusted to 0\n    # Plot a countplot for the current categorical column\n    sns.countplot(x=filtered_categorical_columns[i], data=trans_detail, palette='winter',\n                  order=trans_detail[filtered_categorical_columns[i]].value_counts().index, ax=ax)\n    \n    # Rotate x-axis labels for better readability\n    ax.tick_params(axis='x', rotation=45)\n    \n    # Add values on top of each bar\n    for p in ax.patches:\n        ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2, p.get_height()), ha='center', va='bottom')\n\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\n\n# Show the plot\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These histograms count the quantity of instances and in the categorical features and express the data distribution in each feature.\n\nThe data distribution of categorical in this dataset has not the striking variation. Exception is the feature 'Country', which is a geographic description, and contains the obvious differences between the values. It means excepting 'Country' feature, the others could be used to divide the dataset for mitigating the noise more readily than 'Country'.\n\n","metadata":{}},{"cell_type":"code","source":"# Initialize a variable to store the count\ncount_online_newsletter = 0\ncount_newsletter = 0\n\n# Iterate through each row in the DataFrame\nfor i in range(len(trans_detail)):\n    # Check if the value of 'Channel' is 'Online' and the value of 'Newsletter' is 'Y'\n    if trans_detail.loc[i, 'Channel'] == 'Online' and trans_detail.loc[i, 'Newsletter'] == 'Y':\n        # Increment the count if the condition is met\n        count_online_newsletter += 1\n    if trans_detail.loc[i, 'Newsletter'] == 'Y':\n        count_newsletter += 1\n        \n\ncount_online_newsletter_rate = count_online_newsletter/count_newsletter*100\n\n# Print the count\nprint(\"Number of occurrences where Channel is 'Online' and Newsletter is 'Y':\", count_online_newsletter)\nprint(f\"Percentage of occurrences where Channel is 'Online' and Newsletter is 'Y', on the number of 'Y': {count_online_newsletter_rate}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These above charts show the number of customers who performed shopping by two means and with or without the suggestion. It poited out the number of sold items will be higher if the customers receive the newsletter of the update. There are no evident to determine the receiving newsletter will lead to shopping online. Because the number of customers who bought products online by newsletter, take just above 50% customers (6.778 customers). However, website assisted the online shopping immediately for approx 50% customers received newsletter. And another half of shopping online for the ones no received newsletter expectedly. Noticeably, the whole transaction of the offline shopping is just minimally above that of the online shopping (respectively 12.644 customers and 12.569 customers).  ","metadata":{}},{"cell_type":"markdown","source":"### <b><u>Revenue</u></b>","metadata":{"id":"Dn9lsy9U8juh"}},{"cell_type":"code","source":"# Set the figure size\nfiguresize = (20, 7)\n\n# Define the number of columns and rows for subplots\ncols = 4\nrows = math.ceil(len(filtered_categorical_columns) / cols)\n\n# Create subplots\nplt.subplots(rows, cols, figsize=figuresize)\n\n# Iterate over the subplots and plot barh plots for each categorical column\nfor i in range(1, len(filtered_categorical_columns) + 1):\n    plt.subplot(rows, cols, i)\n    trans_detail.groupby(filtered_categorical_columns[i - 1])[\"SellPrice\"].sum().sort_values().plot(kind=\"barh\")\n    plt.xticks(rotation=0)\n\n# Adjust the layout to avoid overlapping\nplt.tight_layout()\n\n# Show the plot\nplt.show()","metadata":{"id":"orvxc2oV8juh","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><u>Item sales volume</u></b>","metadata":{}},{"cell_type":"code","source":"figuresize = (20,7)\ncols = 4\nrows = math.ceil(len(filtered_categorical_columns) / cols)\n\nplt.subplots(rows, cols, figsize=figuresize)\nfor i in range(1, len(filtered_categorical_columns)+1):\n    plt.subplot(rows, cols, i)\n    trans_detail.groupby(filtered_categorical_columns[i-1])[\"ItemID\"].count().sort_values().plot(kind=\"barh\")\n    plt.xticks(rotation=0)\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"A4oWwgut8juh","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><u>Customer volume</u></b>","metadata":{}},{"cell_type":"code","source":"figuresize = (20,7)\ncols = 4\nrows = math.ceil(len(filtered_categorical_columns) / cols)\n\nplt.subplots(rows, cols, figsize=figuresize)\nfor i in range(1, len(filtered_categorical_columns)+1):\n    plt.subplot(rows, cols, i)\n    trans_detail.groupby(filtered_categorical_columns[i-1])[\"CustomerID\"].nunique().sort_values().plot(kind=\"barh\")\n    plt.xticks(rotation=90)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### C. <b><u>Numerical feature</u></b>\n","metadata":{"id":"nYzaV8rR8juj"}},{"cell_type":"code","source":"numerical_columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filtered_numerical_columns = numerical_columns.drop(['OrderID','CustomerID','ItemID','ordinal'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Initialize the StandardScaler\nscaler = StandardScaler()\n\n# Fit and transform the data\nscaled_data = scaler.fit_transform(trans_detail[numerical_columns])\n\n# Convert the result back to a DataFrame (if needed)\nscaled_df = pd.DataFrame(scaled_data, columns=numerical_columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figuresize = (20,10)\ncols = 3\nrows = math.ceil(len(filtered_numerical_columns) / cols)\n\nplt.subplots(rows, cols, figsize=figuresize)\nfor i in range(1, len(filtered_numerical_columns) + 1):\n    plt.subplot(rows, cols, i)\n    sns.histplot(trans_detail[filtered_numerical_columns[i-1]], color='blue', edgecolor='black', kde=True, bins=20)\n    plt.xticks(rotation=90)\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"w88Nd1RP8juj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figuresize = (20,10)\ncols = 3\nrows = math.ceil(len(filtered_numerical_columns) / cols)\n\nfig, axes = plt.subplots(rows, cols, figsize=figuresize)\nfig.suptitle('Boxplots of Numerical Columns', fontsize=16)  # Add main title to the figure\n\nfor i, col in enumerate(filtered_numerical_columns):\n    row_idx = i // cols\n    col_idx = i % cols\n    sns.boxplot(trans_detail[col], ax=axes[row_idx, col_idx], color='blue')\n    axes[row_idx, col_idx].set_title(col)  # Add subplot title\n    axes[row_idx, col_idx].tick_params(axis='x', rotation=90)\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"0NM-LTAE8juj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trans_detail[filtered_numerical_columns].describe()","metadata":{"id":"6qPD56LG8juj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pairplot\nsns.pairplot(trans_detail[filtered_numerical_columns], diag_kind=\"kde\", )\nplt.show()","metadata":{"id":"nmPPA6aQ8juk","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(trans_detail[filtered_numerical_columns].corr(), annot=True, cmap=\"winter\", linewidths=0.2)\nplt.gcf().set_size_inches(18, 14)\n\nplt.show()","metadata":{"id":"vP1_c54T8juk","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Regarding the data distribution, the findings were collude by these above graphs, include:\n\n1. There are three out of eight numerical feature which has been skewed nearly total to the left due to the numerous long upper wick, are 'uniquePaperviews','Paperviews', and 'Users' features illustrating the web traffic object. It means the majority of data cluster at the cap, but the minority scattered at the very higher volume. Notably, these three features share the same distribution pattern and illustrate the \"web traffic\" object.\n2. The relationship duration illustrate the object has been also skewed totally to the left, but the majority distributed during large extent under the middle with many upper outliners and poor lower outliners. The others have the moderately equal distribution for each part numerical value serie.\n\n\nIn terms of the correlation between each pair feature collaborated, these graphs show that there correlational impact between each feature with the others. The findings were concluded by these above graphs, include:\n\n1. The numerical features which illustrates the 'customer' object, divide data at the others by obvious constraint. They express no trendy, the ambiguous interaction, reversely, the classification is expressed clearly. In addition, they also express the almost non-correlation with others. A similar pattern was witnessed in the numerical features of 'Product' object.\n\n2. The numerical features of 'web traffic' witnessed a small variation. Their values show a stronger correlational impact between it and the other objects. ","metadata":{"id":"MUzwSttg8juk"}},{"cell_type":"markdown","source":"## <b><u>VI. Feature selecting & Modeling</u></b>\n### 1. <b><u>How web traffic impacts on business performance? </u></b>\nTo deal with this request, firstly, we have to determine the proper features illustrate obviously the material and consequent objects, which will be used to model the correlational impact.\n\nIn this request, the matter is 'web traffic' object and the outcome is the number of issue being relevant to transaction. The detai is below:\n\n* Material features : x = 'uniquePageviews', 'existing_duration','users'.\n\n* Outcome features: y = 'item_sales_volume', 'customer_conversion_rate' and 'customer_volime'.\n\n**Note:** The material features are all features of 'traffic' object. However, the consequent features are the new feature are depended on their value and the value united by material feature of the fact table. Moreover, there are many features which describe one object, would be use intermittently to evaluate sales performance, but are depended meticulously on each other. Thus, this project just focus on features that are the least depended to reduce complex and optimize performance, are 'item_sales_volume', 'customer_conversion_rate' and 'relationship duration'. A similar pattern was applied for material features, the choices are 'uniquePageviews', 'existing_duration', 'Posted On (DD/MM/YYYY)','Channel','Newsletter'.\n\n**Method:** Visualization the correlation between each depended feature with each the independed one.\n\n#### A. **Users:**\n* **<b><u>Item sales volume**","metadata":{}},{"cell_type":"code","source":"# Group the data by Product and calculate others\ngroupby_product = trans_detail.groupby(['Product'], as_index=False).agg(uniquePageviews= ('uniquePageviews','mean'),\n                                                                        existing_duration=('existing_duration','mean'),\n                                                                        user_volume = ('users','mean'),\n                                                                        order_volume = ('OrderID','nunique'),\n                                                                        product_sales_volume=('ItemID', 'count'),\n                                                                        revenue= ('SellPrice', 'sum'),\n                                                                        customer_volume=('CustomerID', 'nunique'))\n\ntotall_users = groupby_product['user_volume']\ngroupby_product['conversion_rate']= totall_users/groupby_product['order_volume']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sets up the figure size and create subplots\nplt.figure(figsize=(18, 10))\n\n# Create scatter plot\nsns.scatterplot(x='user_volume', y='product_sales_volume',  hue='revenue', size='customer_volume',\n                data=groupby_product, palette='winter', edgecolor='black')\n\n# Add trend line\nsns.regplot(x='user_volume', y='product_sales_volume', data=groupby_product, scatter=False, color='orchid', line_kws={\"linewidth\": 1})\n    \nplt.xlabel('User volume')\nplt.ylabel('Product sales volume')\nplt.title(f'Scatter Plot by Product')\nplt.xticks(rotation=0)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This chart points out am ambigious interaction between the number of product's sold items and webpage's users. \n\nHowever, all items are on stock of the company, were sold at least once time. The products approached upper 40.000 webpage's users, frequently, reach more than one transaction. \n\nMaybe, this is the effort of company to deal with the inventory.","metadata":{}},{"cell_type":"markdown","source":"* **<b><u>Customer conversion rate:**","metadata":{}},{"cell_type":"code","source":"# Sets up the figure size and create subplots\nplt.figure(figsize=(18, 10))\n\n# Create scatter plot\nsns.scatterplot(x='user_volume', y='conversion_rate',  hue='product_sales_volume', size='customer_volume',\n                data=groupby_product, palette='winter', edgecolor='black')\n\n# Add trend line\nsns.regplot(x='user_volume', y='conversion_rate', data=groupby_product, scatter=False, color='orchid', line_kws={\"linewidth\": 1})\n    \nplt.xlabel('User volume')\nplt.ylabel('Conversion rate')\nplt.title(f'Scatter Plot by Product')\nplt.xticks(rotation=0)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=traffic['users'].sum()\ny=trans_detail['OrderID'].nunique()\nprint(x)\nprint(y)\nprint(f\"General customer conversion rate: {x/y} users/ 1 customer\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By the above description, we could realize that ","metadata":{}},{"cell_type":"markdown","source":"* **<b><u>Relationship duration:**","metadata":{}},{"cell_type":"markdown","source":"We can smoothly reallize that customers having equal value with all feature. It means the application of website and Mail-Chimp is satisfying the customer expectation with the same indicatior between higher and lower applcation. ","metadata":{}},{"cell_type":"markdown","source":"#### B. **Webpage existing duration:**\n* **<b><u>Item sales volume**","metadata":{}},{"cell_type":"code","source":"# Sets up the figure size and create subplots\nplt.figure(figsize=(18, 10))\n    \n# Group the data by date and calculate the sum of installs and count of apps\ngroupby_date = trans_detail.groupby(['Product'], as_index=False).agg(existing_duration=('existing_duration','mean'),\n                                                                               product_sales_vol=('ordinal', 'count'),\n                                                                            revenue= ('SellPrice', 'sum'),\n                                                                            cust_vol=('CustomerID', 'nunique'))\n\n# Create scatter plot\nsns.scatterplot(x='existing_duration', y='product_sales_vol',  hue='revenue', size='cust_vol',\n                data=groupby_date, palette='winter', edgecolor='black')\n\n# Add trend line\nsns.regplot(x='existing_duration', y='product_sales_vol', data=groupby_date, scatter=False, color='orchid', line_kws={\"linewidth\": 1})\n    \nplt.xlabel('Webpage existing duration')\nplt.ylabel('Product sales volume')\nplt.title(f'Scatter Plot by Product')\nplt.xticks(rotation=0)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sets up the figure size and create subplots\nplt.figure(figsize=(18, 10))\n    \n# Group the data by date and calculate the sum of installs and count of apps\ngroupby_date = trans_detail.groupby(['existing_duration'], as_index=False).agg(product_sales_vol=('ordinal', 'count'),\n                                                                            revenue= ('SellPrice', 'sum'),\n                                                                            cust_vol=('CustomerID', 'nunique'))\n\n# Create scatter plot\nsns.scatterplot(x='existing_duration', y='product_sales_vol',  hue='revenue', size='cust_vol',\n                data=groupby_date, palette='winter', edgecolor='black')\n\n# Add trend line\nsns.regplot(x='existing_duration', y='product_sales_vol', data=groupby_date, scatter=False, color='orchid', line_kws={\"linewidth\": 1})\n    \nplt.xlabel('Webpage existing duration')\nplt.ylabel('Product sales volume')\nplt.title(f'Scatter Plot by Webpage existing duration')\nplt.xticks(rotation=0)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This mixed chart points out the the webpage' existing duration has the visible impact on 'item_sales_volume', revenue and customer volume. The mean number of sold item climbed from just under 70 to just under 80 with the higher webpage's existing duration. ","metadata":{}},{"cell_type":"markdown","source":"* **<b><u>Customer conversion rate:**","metadata":{}},{"cell_type":"code","source":"# Sets up the figure size and create subplots\nplt.figure(figsize=(18, 10))\n    \n# Group the data by date and calculate the sum of installs and count of apps\ngroupby_product = trans_detail.groupby(['Product'], as_index=False).agg(existing_duration=('existing_duration','mean'),\n                                                                        uniquePageviews= ('uniquePageviews','mean'),\n                                                                    user_vol=('users', 'mean'),\n                                                                    order_vol=('OrderID','nunique'),\n                                                                    customer_vol= ('CustomerID', 'nunique'),\n                                                                    product_sales_vol=('ordinal', 'nunique'),\n                                                                    revenue= ('SellPrice', 'sum'))\n\ntotall_users = groupby_product['user_vol']\ngroupby_product['conversion_rate']= totall_users/groupby_product['order_vol']\n\n# Create scatter plot\nsns.scatterplot(x='existing_duration', y='conversion_rate',  hue='revenue', size='customer_vol',\n                data=groupby_product, palette='winter', edgecolor='black')\n\n# Add trend line\nsns.regplot(x='existing_duration', y='conversion_rate', data=groupby_product, scatter=False, color='orchid', line_kws={\"linewidth\": 1})\n    \nplt.xlabel('Webpage existing duration')\nplt.ylabel('Custome conversion rate')\nplt.title(f'Scatter Plot by Product')\nplt.xticks(rotation=0)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sets up the figure size and create subplots\nplt.figure(figsize=(18, 10))\n    \n# Group the data by date and calculate the sum of installs and count of apps\ngroupby_date = trans_detail.groupby(['existing_duration'], as_index=False).agg(existing_duration=('existing_duration','mean'),\n                                                                        uniquePageviews= ('uniquePageviews','mean'),\n                                                                    user_vol=('users', 'mean'),\n                                                                    order_vol=('OrderID','nunique'),\n                                                                    customer_vol= ('CustomerID', 'nunique'),\n                                                                    product_sales_vol=('ordinal', 'nunique'),\n                                                                    revenue= ('SellPrice', 'sum'))\n\ntotall_users = groupby_product['user_vol']\ngroupby_product['conversion_rate']= totall_users/groupby_product['order_vol']\n\n# Create scatter plot\nsns.scatterplot(x='existing_duration', y='conversion_rate',  hue='revenue', size='customer_vol',\n                data=groupby_product, palette='winter', edgecolor='black')\n\n# Add trend line\nsns.regplot(x='existing_duration', y='conversion_rate', data=groupby_product, scatter=False, color='orchid', line_kws={\"linewidth\": 1})\n    \nplt.xlabel('Webpage existing duration')\nplt.ylabel('Custome conversion rate')\nplt.title(f'Scatter Plot by Webpage existing duration')\nplt.xticks(rotation=0)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In constrast, the webpage's existing duration witnessed a minimal decrease treding with the higher webpage'existing duration. Howerver, the higher webpage'existing duration witnessed the the breakthrough of the number of orders and customers. It poits out there is some negative impact of  the webpage'existing duration on customer coversion.","metadata":{}},{"cell_type":"markdown","source":"* **<b><u>Relationship duration:**","metadata":{}},{"cell_type":"code","source":"# Sets up the figure size and create subplots\nplt.figure(figsize=(18, 10))\n    \n# Group the data by date and calculate the sum of installs and count of apps\ngroupby_date = trans_detail.groupby(['Product'], as_index=False).agg(existing_duration=('existing_duration','mean'),\n                                                                    relationship_duration=('relationship_duration', 'mean'),\n                                                                    user_vol=('users', 'mean'),\n                                                                    customer_vol= ('CustomerID', 'nunique'),\n                                                                    product_sales_vol=('ordinal', 'count'),\n                                                                    revenue= ('SellPrice', 'sum'),\n                                                                    cust_vol=('CustomerID', 'nunique'))\n\n# Create scatter plot\nsns.scatterplot(x='existing_duration', y='relationship_duration',  hue='revenue', size='cust_vol',\n                data=groupby_date, palette='winter', edgecolor='black')\n\n# Add trend line\nsns.regplot(x='existing_duration', y='relationship_duration', data=groupby_date, scatter=False, color='orchid', line_kws={\"linewidth\": 1})\n    \nplt.xlabel('Webpage existing duration')\nplt.ylabel('Relationship duration')\nplt.title(f'Scatter Plot by Product')\nplt.xticks(rotation=0)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sets up the figure size and create subplots\nplt.figure(figsize=(18, 10))\n    \n# Group the data by date and calculate the sum of installs and count of apps\ngroupby_date = trans_detail.groupby(['existing_duration'], as_index=False).agg(relationship_duration=('relationship_duration', 'mean'),\n                                                                            OrderID= ('OrderID', 'nunique'),\n                                                                            cust_vol=('CustomerID', 'nunique'))\n\n# Create scatter plot\nsns.scatterplot(x='existing_duration', y='relationship_duration',  hue='OrderID', size='cust_vol',\n                data=groupby_date, palette='winter', edgecolor='black')\n\n# Add trend line\nsns.regplot(x='existing_duration', y='relationship_duration', data=groupby_date, scatter=False, color='orchid', line_kws={\"linewidth\": 1})\n    \nplt.xlabel('Webpage existing duration')\nplt.ylabel('Relationship duration')\nplt.title(f'Scatter Plot by Webpage existing duration')\nplt.xticks(rotation=0)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trans_detail['relationship_duration'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This mixed chart points out there is no obvious impact of the webpage'existing duration on customer coversion. The meaning is expreesed vaguely.\n\nHowever, combining the decriptive statistical table and the mixed chart we can see the customers duration scattering numerously in the most recent year. It means the numer of customer joining grew up from launching the website. ","metadata":{}},{"cell_type":"markdown","source":"#### C. **Unique page view:**\n* **<b><u>Item sales volume**","metadata":{}},{"cell_type":"code","source":"# Sets up the figure size and create subplots\nplt.figure(figsize=(18, 10))\n    \n# Group the data by date and calculate the sum of installs and count of apps\ngroupby_product=trans_detail.groupby(['Product'], as_index=False).agg(uniquePageviews= ('uniquePageviews','mean'),\n                                                                     existing_duration=('existing_duration','mean'),\n                                                                    relationship_duration=('relationship_duration', 'mean'),\n                                                                    user_vol=('users', 'mean'),\n                                                                    customer_vol= ('CustomerID', 'nunique'),\n                                                                    product_sales_vol=('ordinal', 'nunique'),\n                                                                    revenue= ('SellPrice', 'sum'))\n\n# Create scatter plot\nsns.scatterplot(x='uniquePageviews', y='product_sales_vol',  hue='revenue', size='customer_vol',\n                data=groupby_product, palette='winter', edgecolor='black')\n\n# Add trend line\nsns.regplot(x='uniquePageviews', y='product_sales_vol', data=groupby_product, scatter=False, color='orchid', line_kws={\"linewidth\": 1})\n\nplt.xlabel('Unique page views')\nplt.ylabel('Product sales volume')\nplt.title(f'Scatter Plot by Product')\nplt.xticks(rotation=0)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sets up the figure size and create subplots\nplt.figure(figsize=(18, 10))\n    \n# Group the data by date and calculate the sum of installs and count of apps\ngroupby_numeric=trans_detail.groupby(['uniquePageviews'], as_index=False).agg(uniquePageviews= ('uniquePageviews','mean'),\n                                                                     existing_duration=('existing_duration','mean'),\n                                                                    relationship_duration=('relationship_duration', 'mean'),\n                                                                    user_vol=('users', 'mean'),\n                                                                    customer_vol= ('CustomerID', 'nunique'),\n                                                                    product_sales_vol=('ordinal', 'nunique'),\n                                                                    revenue= ('SellPrice', 'sum'))\n\n# Create scatter plot\nsns.scatterplot(x='uniquePageviews', y='product_sales_vol',  hue='revenue', size='customer_vol',\n                data=groupby_product, palette='winter', edgecolor='black')\n\n# Add trend line\nsns.regplot(x='uniquePageviews', y='product_sales_vol', data=groupby_product, scatter=False, color='orchid', line_kws={\"linewidth\": 1})\n\nplt.xlabel('Unique page views')\nplt.ylabel('Product sales volume')\nplt.title(f'Scatter Plot by Unique page views')\nplt.xticks(rotation=0)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **<b><u>Customer conversion rate:**","metadata":{}},{"cell_type":"code","source":"# Sets up the figure size and create subplots\nplt.figure(figsize=(18, 10))\n    \n# Group the data by date and calculate the sum of installs and count of apps\ngroupby_numeric=trans_detail.groupby(['Product'], as_index=False).agg(uniquePageviews= ('uniquePageviews','mean'),\n                                                                    user_vol=('users', 'mean'),\n                                                                    order_vol=('OrderID','nunique'),\n                                                                    customer_vol= ('CustomerID', 'nunique'),\n                                                                    product_sales_vol=('ordinal', 'nunique'),\n                                                                    revenue= ('SellPrice', 'sum'))\n\ntotall_users = groupby_numeric['user_vol']\ngroupby_numeric['conversion_rate']= totall_users/groupby_numeric['order_vol']\n\n# Create scatter plot\nsns.scatterplot(x='uniquePageviews', y='conversion_rate',  hue='revenue', size='customer_vol',\n                data=groupby_numeric, palette='winter', edgecolor='black')\n\n# Add trend line\nsns.regplot(x='uniquePageviews', y='conversion_rate', data=groupby_numeric, scatter=False, color='orchid', line_kws={\"linewidth\": 1})\n\nplt.xlabel('Unique page views')\nplt.ylabel('Customer coversion rate')\nplt.title(f'Scatter Plot by Product')\nplt.xticks(rotation=0)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sets up the figure size and create subplots\nplt.figure(figsize=(18, 10))\n    \n# Group the data by date and calculate the sum of installs and count of apps\ngroupby_numeric=trans_detail.groupby(['uniquePageviews'], as_index=False).agg(uniquePageviews= ('uniquePageviews','mean'),\n                                                                    user_vol=('users', 'mean'),\n                                                                    order_vol=('OrderID','nunique'),\n                                                                    customer_vol= ('CustomerID', 'nunique'),\n                                                                    product_sales_vol=('ordinal', 'nunique'),\n                                                                    revenue= ('SellPrice', 'sum'))\n\ntotall_users = groupby_numeric['user_vol']\ngroupby_numeric['conversion_rate']= totall_users/groupby_numeric['order_vol']\n\n# Create scatter plot\nsns.scatterplot(x='uniquePageviews', y='conversion_rate',  hue='revenue', size='customer_vol',\n                data=groupby_numeric, palette='winter', edgecolor='black')\n\n# Add trend line\nsns.regplot(x='uniquePageviews', y='conversion_rate', data=groupby_numeric, scatter=False, color='orchid', line_kws={\"linewidth\": 1})\n\nplt.xlabel('Unique page views')\nplt.ylabel('Customer coversion rate')\nplt.title(f'Scatter Plot by \"uniquePageviews\"')\nplt.xticks(rotation=0)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **<b><u>Relationship duration:**","metadata":{}},{"cell_type":"code","source":"# Sets up the figure size and create subplots\nplt.figure(figsize=(18, 10))\n    \n# Group the data by date and calculate the sum of installs and count of apps\ngroupby_numeric=trans_detail.groupby(['Product'], as_index=False).agg(uniquePageviews= ('uniquePageviews','mean'),\n                                                                    relationship_duration=('relationship_duration', 'mean'),\n                                                                    customer_vol= ('CustomerID', 'nunique'),\n                                                                    product_sales_vol=('ordinal', 'nunique'),\n                                                                    revenue= ('SellPrice', 'sum'))\n\n# Create scatter plot\nsns.scatterplot(x='uniquePageviews', y='relationship_duration',  hue='revenue', size='customer_vol',\n                data=groupby_numeric, palette='winter', edgecolor='black')\n\n# Add trend line\nsns.regplot(x='uniquePageviews', y='relationship_duration', data=groupby_numeric, scatter=False, color='orchid', line_kws={\"linewidth\": 1})\n\nplt.xlabel('Unique page views')\nplt.ylabel('Relationship duration')\nplt.title(f'Scatter Plot by Product')\nplt.xticks(rotation=0)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sets up the figure size and create subplots\nplt.figure(figsize=(18, 10))\n    \n# Group the data by date and calculate the sum of installs and count of apps\ngroupby_numeric=trans_detail.groupby(['uniquePageviews'], as_index=False).agg(uniquePageviews= ('uniquePageviews','mean'),\n                                                                    relationship_duration=('relationship_duration', 'mean'),\n                                                                    customer_vol= ('CustomerID', 'nunique'),\n                                                                    product_sales_vol=('ordinal', 'nunique'),\n                                                                    revenue= ('SellPrice', 'sum'))\n\n# Create scatter plot\nsns.scatterplot(x='uniquePageviews', y='relationship_duration',  hue='revenue', size='customer_vol',\n                data=groupby_numeric, palette='winter', edgecolor='black')\n\n# Add trend line\nsns.regplot(x='uniquePageviews', y='relationship_duration', data=groupby_numeric, scatter=False, color='orchid', line_kws={\"linewidth\": 1})\n\nplt.xlabel('Unique page views')\nplt.ylabel('Relationship duration')\nplt.title(f'Scatter Plot by \"uniquePageviews\"')\nplt.xticks(rotation=0)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By these charts, it can be seen that the weak impact of the unique pageviews on the depended features. However, surely, there are correlational impacts existing between them. The products have the good traffic, have the better sales performance, for instance, the higher page view have more than one sold item, but not too high. Noticeable, the product having higher unique page views could be defined that have the low coversion rate. \n\nBecause, beside the transactional online, the products are purchased by offline too. In addition, toward clothing, the instore purchasing certainly bring the better customer experience than online purchasing. So, the low page view is also have several sold item.\n\n=> My conclusion is pageview make the good impact on sale performance. It stimulates the purchasing of items which have the low transaction.","metadata":{}},{"cell_type":"markdown","source":"### 2. <b><u>Customer segmentation </u></b>\nSimilar to the first request, the initial step is to deal with this request is determining the fundamental and consequent objects.\n\nIn this request, the matter could be all numerical, categorical and datetime feature. The exception is the feature that characterise the identity, unique, distinct value.\n\nThe main means which is exploited for the segmentation, is classification splitting and merging data by the quitely resemblance.\n\nThe outcome are the new columns which classificate the customers by the ranked group.","metadata":{}},{"cell_type":"code","source":"customer_summary = trans_detail.groupby('CustomerID').agg(total_orders=('OrderID','count'),total_revenue = ('SellPrice','sum'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_data = customers.merge(customer_summary,left_on = 'ID',right_on = 'CustomerID',how = 'inner')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><u>Loyalty:","metadata":{}},{"cell_type":"code","source":"import datetime\n\ncustomer_data['Loyalty'] = ''\ni = 0\nfor i in range(len(customer_data)):\n    customer_data.loc[[i],'Loyalty'] = date_today.day - datetime.datetime.date(customer_data['DateJoined'][i]).day","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_data['Loyalty'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><u>Monetary:","metadata":{}},{"cell_type":"code","source":"customer_data['Monetary'] = customer_data.apply(lambda x: '1' if (x['total_revenue'] < 14905)\n                                   else ('2' if (x['total_revenue'] >= 14905 and x['total_revenue'] < 26080)\n                                         else ('3' if (x['total_revenue'] >= 26080 and x['total_revenue'] < 36911)\n                                               else '4')), axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_data['Monetary']=customer_data['Monetary'].astype(int)\ncustomer_data['Monetary'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><u>Demographic (Youth):","metadata":{}},{"cell_type":"code","source":"customer_data['Youth'] = customer_data.apply(lambda x: '1' if (x['Age'] < 22)\n                                   else ('2' if (x['Age'] >= 22 and x['Age'] < 32)\n                                         else ('3' if (x['Age'] >= 32 and x['Age'] < 43)\n                                               else '4')), axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_data['Youth']=customer_data['Youth'].astype(int)\ncustomer_data['Youth'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><u>Demographic (Gender):","metadata":{}},{"cell_type":"code","source":"customer_data['Gender'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_data['Gender'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><u>Geographic (Country/City):","metadata":{}},{"cell_type":"code","source":"customer_data['Country'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_data['Country'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><u>Receiving newsletter:","metadata":{}},{"cell_type":"code","source":"customer_data['Newsletter'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_data['Newsletter'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><u>Posh & Trending:","metadata":{}},{"cell_type":"code","source":"trans_detail.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"itemsRevenue = trans_detail.groupby(['Product','Brand']).agg(totalRevenue = ('SellPrice','sum'),\n                                                        totalOrders=('OrderID','nunique'),\n                                                        totalCustomers = ('CustomerID','nunique'),\n                                                        totalItems = ('ItemID','count'))\nitemsRevenue.reset_index(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"itemsRevenue.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(data=itemsRevenue,y='totalRevenue')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"itemsRevenue.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"itemsRevenue['Ranking'] = itemsRevenue.apply(lambda x: 'Low' if (x['totalRevenue'] < 742)\n                                   else ('Medium' if (x['totalRevenue'] >= 742 and x['totalRevenue'] < 1186)\n                                         else ('High' if (x['totalRevenue'] >= 1186 and x['totalRevenue'] < 2042)\n                                               else 'Cash Cow')), axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"itemsRevenue.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"itemsTraffic = trans_detail.groupby(['Brand','Product']).agg(totalUsers = ('users','sum'),\n                                                        totalUniquePageviews=('uniquePageviews','sum'),\n                                                        totalPageviews = ('pageviews','sum'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(data=itemsTraffic,y='totalPageviews')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"itemsTraffic.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"itemsTraffic['trafficRanking'] = itemsTraffic.apply(lambda x: 'Low' if (x['totalPageviews'] < 358)\n                                   else ('Medium' if (x['totalPageviews'] >= 358 and x['totalPageviews'] < 683)\n                                         else ('High' if (x['totalPageviews'] >= 683 and x['totalPageviews'] < 1499)\n                                               else 'Most Popular')), axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"itemsTraffic","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"itemsTraffic= itemsTraffic.reset_index(level=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"itemsTraffic","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"itemsTraffic.reset_index(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"itemsTraffic","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"itemsRevenue","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"revenue_popularity = itemsRevenue.merge(itemsTraffic,on = ['Product','Brand'],\n                                         how = 'inner').sort_values(['totalRevenue','totalPageviews'],ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"revenue_popularity.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"revenue_popularity = revenue_popularity[['Brand','Product','totalRevenue','totalPageviews','Ranking','trafficRanking']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"revenue_popularity","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(revenue_popularity.groupby(['trafficRanking','Ranking'])['Product'].count())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"revenue_popularity[(revenue_popularity['trafficRanking'] == 'Most Popular') & (revenue_popularity['Ranking'] == 'Cash Cow')].sort_values(['totalRevenue','totalPageviews'],ascending=False).head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><u> CLV (Customer lifetim value):","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><u>RFM (Recency-Frequency-Monetary):","metadata":{}},{"cell_type":"code","source":"recency_df= pd.DataFrame(df.groupby(by='CustomerID', as_index=False)['TransactionDate'].max())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recency_df['Recency']= df['TransactionDate'].apply(lambda x: (date_today - x).days)\nrecency_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frequency_df = pd.DataFrame(df.groupby([\"CustomerID\"]).agg({\"OrderID\":\"nunique\"}).reset_index())\nfrequency_df.rename(columns={\"OrderID\":\"Frequency\"}, inplace=True)\nfrequency_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monetary_df = trans_detail.groupby('CustomerID', as_index=False)['SellPrice'].sum()\nmonetary_df.rename(columns={\"SellPrice\":\"Monetary\"}, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfRF = recency_df.merge(frequency_df, on='CustomerID')\ndfRFM = dfRF.merge(monetary_df, on='CustomerID')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfRFM[\"recency_score\"]  = pd.qcut(dfRFM['Recency'], 5, labels=[5, 4, 3, 2, 1])\ndfRFM[\"frequency_score\"]= pd.qcut(dfRFM['Frequency'].rank(method=\"first\"), 5, labels=[1, 2, 3, 4, 5])\ndfRFM[\"monetary_score\"] = pd.qcut(dfRFM['Monetary'], 5, labels=[1, 2, 3, 4, 5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfRFM['RFM_SCORE'] = dfRFM.recency_score.astype(str)+ dfRFM.frequency_score.astype(str) + dfRFM.monetary_score.astype(str)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Customers = customers.merge(dfRFM, left_on='ID',right_on='CustomerID')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RFM_seg_map= {\n    r'111|112|121|131|141|151': 'Lost customers',\n    r'332|322|233|232|223|222|132|123|122|212|211': 'Hibernating customers',\n    r'155|154|144|214|215|115|114|113': 'Cannot Lose Them',\n    r'255|254|245|244|253|252|243|242|235|234|225|224|153|152|145|143|142|135|134|133|125|124': 'At Risk',\n    r'331|321|312|221|213|231|241|251': 'About To Sleep',\n    r'535|534|443|434|343|334|325|324': 'Need Attention',\n    r'525|524|523|522|521|515|514|513|425|424|413|414|415|315|314|313': 'Promising',\n    r'512|511|422|421|412|411|311': 'New Customers',\n    r'553|551|552|541|542|533|532|531|452|451|442|441|431|453|433|432|423|353|352|351|342|341|333|323': 'Potential Loyalist',\n    r'543|444|435|355|354|345|344|335': 'Loyal',\n    r'555|554|544|545|454|455|445': 'Champions'\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Customers['Segment'] = Customers['RFM_SCORE'].replace(RFM_seg_map, regex=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Customers[(Customers['Segment'] == 'Champions')].sort_values(['Monetary','Recency'],ascending=False).head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfCustStats = Customers[[\"Segment\", \"Recency\", \"Frequency\", \"Monetary\"]].groupby(\"Segment\").agg(['mean','median', 'min', 'max', 'count'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install squarify\nimport squarify","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Treemap by recency/frequency\nplt.figure(figsize=(15,8))\nplt.rc('font', size=15)\nsquarify.plot(sizes=dfCustStats[\"Recency\"][\"count\"], label=dfCustStats.index,\n              color=[\"red\",\"orange\",\"blue\", \"forestgreen\", \"yellow\", \"purple\", \"cornsilk\",\"royalblue\", \"pink\", \"brown\"], alpha=.55)\nplt.suptitle(\"Recency and Frequency Grid\", fontsize=25);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><u>Using Clustering model of Machine Learning","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfCluster = customer_data[['Loyalty','Monetary','Youth']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans = KMeans(init=\"random\",n_clusters=3,n_init=10,max_iter=300,random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans.fit(dfCluster)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from yellowbrick.cluster import KElbowVisualizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k_means = KMeans()\nelbow = KElbowVisualizer(k_means, k=(2, 20))\nelbow.fit(dfCluster)\nelbow.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans= KMeans(n_clusters=elbow.elbow_value_)\nkmeans.fit(dfCluster)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfCluster['Cluster']= kmeans.labels_\ndfCluster.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfStats = dfCluster.groupby(\"Cluster\").agg(['mean','median', 'min', 'max', 'count'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfStats.style.background_gradient(cmap='YlGnBu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <b><u>VII. Model training</u></b>\n### 1. <b><u>How web traffic impacts on business performance? </u></b>\n* Diagnostic prediction by Regression models of Machine Learning.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report","metadata":{"id":"_IKshCLn8jul","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trans_detail_filtered= trans_detail.groupby('Product').agg(uniquePageviews=('uniquePageviews', 'mean'), \n                                                            existing_duration=('existing_duration', 'mean'),\n                                                            product_sales_vol=('ItemID','count')).reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=trans_detail_filtered[['uniquePageviews','existing_duration']]\ny=trans_detail_filtered['product_sales_vol']","metadata":{"id":"fx3OgMTB8jum","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Random Frorest Regressor","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n# Split the data for each target variable\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\n# Initialize the Random Forest Regressor for y\nreg_y = RandomForestRegressor()\n# Train the regressor for y\nreg_y.fit(x_train, y_train)\n# Make predictions for y\ny_pred = reg_y.predict(x_test)","metadata":{"id":"1H2FG__T8jum","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n# Calculate Mean Absolute Error (MAE)\nmae_y = mean_absolute_error(y_test, y_pred)\nprint(\"Mean Absolute Error for y:\", mae_y)\n\n# Calculate Mean Squared Error (MSE)\nmse_y = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error for y1:\", mse_y)\n\n# Calculate R-squared (R^2) score\nr2_y = r2_score(y_test, y_pred)\nprint(\"R-squared (R^2) score for y:\", r2_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot actual vs. predicted values for y\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, y_pred, color='blue', alpha=0.5)\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\nplt.title('Actual vs. Predicted for y')\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n# Initialize the Random Forest Regressor for y\nreg_y = RandomForestRegressor()\n\n# Train the regressor for y\nreg_y.fit(x_train, y_train)\n\n# Get feature importances\nfeature_importances = reg_y.feature_importances_\n\n# Print feature importances\nfor i, feature in enumerate(x.columns):\n    print(f'{feature}: {feature_importances[i]}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In summary, these metrics suggest that the model is not performing well in predicting the dependent variable based on the independent variables. Further improvements to the model or consideration of other factors may be necessary to enhance its accuracy.","metadata":{}},{"cell_type":"markdown","source":"#### Linear Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\n# Split the data for each target variable\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\n# Initialize the Linear Regression model for y\nreg_y = LinearRegression()\n\n# Train the regressor for y\nreg_y.fit(x_train, y_train)\n\n# Make predictions for y\ny_pred = reg_y.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n# Calculate Mean Absolute Error (MAE)\nmae_y = mean_absolute_error(y_test, y_pred)\nprint(\"Mean Absolute Error for y:\", mae_y)\n\n# Calculate Mean Squared Error (MSE)\nmse_y = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error for y1:\", mse_y)\n\n# Calculate R-squared (R^2) score\nr2_y = r2_score(y_test, y_pred)\nprint(\"R-squared (R^2) score for y:\", r2_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot actual vs. predicted values for y\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, y_pred, color='blue', alpha=0.5)\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\nplt.title('Actual vs. Predicted for y')\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\n# Initialize the Linear Regression model for y\nreg_y = LinearRegression()\n\n# Train the model for y\nreg_y.fit(x_train, y_train)\n\n# Get coefficients\ncoefficients = reg_y.coef_\n\n# Print coefficients\nfor i, feature in enumerate(x.columns):\n    print(f'{feature}: {coefficients[i]}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In summary, these metrics suggest that the model is not performing well in predicting the dependent variable based on the independent variables. Further improvements to the model or consideration of other factors may be necessary to enhance its accuracy.","metadata":{}},{"cell_type":"markdown","source":"#### Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n# Split the data for each target variable\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\n# Initialize the Logistic Regression model\nlog_reg_model = LogisticRegression()\n\n# Train the model\nlog_reg_model.fit(x_train, y_train)\n\n# Make predictions\ny_pred = log_reg_model.predict(x_test)\n\n# Evaluate the model\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Absolute Error:\", mae)\nprint(\"Mean Squared Error:\", mse)\nprint(\"R-squared (R^2) score:\", r2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot actual vs. predicted values for y\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, y_pred, color='blue', alpha=0.5)\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\nplt.title('Actual vs. Predicted for y')\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# Initialize the Logistic Regression model for y\nreg_y = LogisticRegression()\n\n# Train the model for y\nreg_y.fit(x_train, y_train)\n\n# Get coefficients\ncoefficients = reg_y.coef_\n\n# Print coefficients\nfor i, feature in enumerate(x.columns):\n    print(f'{feature}: {coefficients[0][i]}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. <b><u>Customer segmentation </u></b>","metadata":{}},{"cell_type":"code","source":"dfCluster = dfCluster[['Loyalty','Monetary','Youth']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install kneed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kneed import KneeLocator","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans_kwargs = {\"init\": \"random\",\"n_init\": 10,\"max_iter\": 300,\"random_state\": 42}\n\nsse = []\nfor k in range(1, 11):\n    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n    kmeans.fit(dfCluster)\n    sse.append(kmeans.inertia_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kl = KneeLocator(range(1, 11), sse, curve=\"convex\", direction=\"decreasing\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kl.elbow","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans = KMeans(n_clusters=4, random_state=0).fit(dfCluster)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfCluster[\"Cluster Labels\"] = kmeans.labels_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfCluster['Cluster Labels'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Overall, based on these scores, we can conclude that the clustering model appears to perform well in terms of cluster separation and cohesion. However, it's essential to consider domain knowledge and the specific context of the data when interpreting these scores.","metadata":{}},{"cell_type":"markdown","source":"## <b><u>Conclusion:</u></b>\nIn terms of the accurace of this research through applying Machine Learning's models, the testing results show the high imbalance in the input data.\n\nThe percentile of models are skewed far away from their median. conseequently, the results of analysis models wich were used to predict the objects, are totally biased towards one side.\n\nThe insight gained from this analysis is there are correlational effects betwween them, however, it just expresses that effect for one dimension of the vector.\n\nParticulartly, toward favorability object (Rating), It is quitely correct that the high initial technical indicators bring the high favored, but when decrease the initial technical indicators.\n\nThe object tend to go out the gap of a correct result. The contrast was witnessed in the model of the popularity (Installs).\n\nTo improve the accuracy of this research, the reseach object should be mined deeper in each categorical features.\n\nMoreover, applying techniques which assist handling imbalanced data, need to be implemented such as resampling or collecting real data, etc.\n","metadata":{"id":"6S_h04Qb8juq"}}]}